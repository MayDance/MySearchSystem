# 信息检索系统

利用倒排索引和向量空间模型实现的信息检索系统。

## 完成工作以及分工：
- 马哲：
    - 语言分析
    - 拼写矫正
- 袁德华：
    - 带位置信息的倒排索引
    - 索引压缩
- 宋鼎：
    - BOOL查询
    - 短语查询
- 姚婧、徐维亚：  
    - 向量空间模型
    - TOP K查询

# 运行
环境要求：python3 
语料库需自行下载放置于`./LanguageAnalysis/Reuters`中，该文件夹中包含所有docID.html文件。

> 注意：运行前请不要修改工程文件的名字和相对位置

运行方式：
```commandline
python main.py
```
## 运行输入与输出
- The first time to load this System?[Y]/[N]
    - 第一次运行需要输入Y，构建向量空间
    - 之后运行请输入N继续
### 布尔查询、短语查询
- input the search method(EXIT to quit):
    - 输入：BOOL
- input the query statement(EXIT to quit):
    - 布尔输入样例（NOT AND OR需要大写）：NOT word1 AND word2 OR word3
    - 短语查询直接输入需要查询短语
- 输出
    - 首先输出拼音矫正结果，再输出查询结果（文档数目+文档列表）
### TOP-K 查询
- input the search method(EXIT to quit):
    - 输入：SCORE
- input the query statement(EXIT to quit):
    - 直接输入需要查询内容
- input the K:
    - 输入查询结果数量
- 输出
    - 拼音矫正结果
    - 文档名+文档得分
    - 文档内容中包含查询项的周围语句
    - 总结：文档数目+文档列表

    

# 实现功能
## 词干还原
利用python中自然语言处理的库：nltk对文章中的单词进行词干还原。

在词干还原的过程中会去除无用的标点符号。

## 索引构建
带位置信息的倒排索引：

例子：
```json
{
    "word1":{
        "1":[5,6,10],
        "5":[10,20,30]
    },
    "word2":{
        "2":[5,6,10],
        "33":[15,28,30]
    }
    ···
}
```
索引事先建立好储存在文件中，每次运行程序时将索引加载到内存中。

## 向量空间模型

根据词项的df计算出idf，根据词项索引中每个doc的tf，计算相应的tf-idf。不在索引中的doc，该词项的tf-idf为0

遍历倒排索引表，可以得到所有tf-idf，即生成了向量空间模型

由于向量空间模型是一个稀疏矩阵，又非常的大，直接生成放在内存非常占用空间。
利用对稀疏矩阵的处理方法，保存这个向量空间模型，存入文件中
每次运行不需要再次生成向量空间，只需要导入即可

## TOP K 查询
首先通过向量空间模型得到所有文档的向量，和查询向量进行余弦相似度计算获得评分。

通过堆排序建好最大堆以后进行K次pop+precDown操作。

将pop得到的K个元素返回。

## 短语查询
利用带位置信息的倒排索引。

首先得到包含query中所有单词的文档列表的交集。

从这些文档集中根据位置索引查找是否有匹配的短语。

遍历第一个词项在文档中的位置，依次检测后面的词项位置中是否包含与其匹配的位置。

## 通配符查询
利用正则表达式找到所有匹配的词项，利用倒排索引检索出词项对应的文档。

支持通配符的短语查询：

首先将检索到的词项存在一个二维数组中，随后对出现的每个短语组合进行短语查询。

## 同义词查询
同样利用nltk语言处理库获取单个单词的同义词列表（有可能是短语）

随后对每个单词或者短语进行检索，获取文档集。

## BOOL查询

- 将查询表达式转为后序表达式：
如 A OR B AND C 转为
     A B C AND OR 

- 用一个栈来计算后序表达式

## 拼写矫正
![](https://raw.githubusercontent.com/TaiyouDong/SearchingSystem/master/img/img1.png)
c 是 编辑距离为1或者2或者0的词，并且词属于给定的词典.

P(c)是每个词在字典中出现的频率.

P(w)是每一种拼写错误可能出现的概率，这里认为是一个常数

根据贝叶斯理论
计算 P(c|w)，即给定词w，纠正到每个c的概率，并且从中选出概率最高的词
这里还需要知道一个项 P(w|c|)，即给定c，c就是输入者想要的单词的概率
由于没有错误模型数据，我们简单这么认为

P(W|C0) >> P(W|C1) >> P(W|C2)

C0/1/2 是编辑距离为0/1/2的词，即编辑距离小的有更高的优先级

最后，模型工作如下
1. 如果输入词在词典里找到了原词，这返回
2. 从编辑距离为1的词选出频率最高的，返回
3. 从编辑距离为2的词选出频率最高的返回。















 
